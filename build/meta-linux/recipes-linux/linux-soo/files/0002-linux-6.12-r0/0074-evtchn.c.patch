--- /home/rossierd/soo/micofe/build/tmp/work/linux-6.12-r0/linux-6.12/soo/kernel/evtchn.c	1970-01-01 01:00:00.000000000 +0100
+++ ./soo/kernel/evtchn.c	2025-08-24 11:46:06.524091167 +0200
@@ -0,0 +1,427 @@
+
+/*
+ * Copyright (C) 2014-2016 Daniel Rossier <daniel.rossier@heig-vd.ch>
+ * Copyright (C) 2016 Baptiste Delporte <bonel@bonel.net>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+ *
+ */
+
+#if 0
+#define DEBUG
+#endif
+
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/version.h>
+#include <linux/slab.h>
+#include <linux/irq.h>
+#include <linux/irqdesc.h>
+#include <linux/irqnr.h>
+#include <linux/irqdomain.h>
+#include <linux/sched.h>
+#include <linux/kernel_stat.h>
+#include <linux/random.h>
+#include <linux/kthread.h>
+ 
+#include <asm/atomic.h>
+#include <asm/ptrace.h>
+#include <asm/irqflags.h>
+
+#include <asm-generic/irq_regs.h>
+
+#include <soo/hypervisor.h>
+#include <soo/evtchn.h>
+
+#include <soo/uapi/soo.h>
+#include <soo/uapi/console.h>
+
+#include <soo/uapi/debug.h>
+
+/* Convenient shorthand for packed representation of an unbound IRQ. */
+#define IRQ_UNBOUND	mk_virq_info(IRQT_UNBOUND, 0, 0)
+
+/*
+ * This lock protects updates to the following mapping and reference-count
+ * arrays. The lock does not need to be acquired to read the mapping tables.
+ */
+static DEFINE_SPINLOCK(virq_mapping_update_lock);
+
+typedef struct {
+	int evtchn_to_virq[NR_EVTCHN];	/* evtchn -> IRQ */
+	u32 virq_to_evtchn[NR_VIRQS];	/* IRQ -> evtchn */
+	bool valid[NR_EVTCHN]; /* Indicate if the event channel can be used for notification for example */
+	bool evtchn_mask[NR_EVTCHN];
+	int virq_bindcount[NR_VIRQS];
+} evtchn_info_t;
+
+static DEFINE_PER_CPU(evtchn_info_t, evtchn_info);
+
+/*
+ * Get the evtchn from a irq_data structure which contains a (unique) CPU bound to the IRQ processing.
+ */
+inline unsigned int evtchn_from_virq_and_cpu(unsigned int virq, unsigned int cpu) {
+	return per_cpu(evtchn_info, cpu).virq_to_evtchn[virq];
+}
+
+inline unsigned int evtchn_from_virq(int virq)
+{
+	return evtchn_from_virq_and_cpu(virq, smp_processor_id());
+}
+
+inline unsigned int evtchn_from_irq_data(struct irq_data *irq_data)
+{
+	return evtchn_from_virq(irq_data->irq - VIRQ_BASE);
+}
+
+/* __evtchn_from_irq : helpful for irq_chip handlers */
+#define __evtchn_from_virq   evtchn_from_irq_data
+
+static inline bool evtchn_is_masked(unsigned int b) {
+	return per_cpu(evtchn_info, smp_processor_id()).evtchn_mask[b];
+}
+
+#if 0 /* debug */
+static void dump_evtchn_pending(void) {
+	int i;
+
+	lprintk("   Evtchn info in Agency CPU %d\n\n", smp_processor_id());
+
+	for (i = 0; i < NR_EVTCHN; i++)
+		lprintk("e:%d m:%d p:%d  ", i, per_cpu(evtchn_info, smp_processor_id()).evtchn_mask[i],
+			avz_shared->evtchn_pending[i]);
+
+	lprintk("\n\n");
+}
+#endif /* 0 */
+
+/* NB. Interrupts are disabled on entry. */
+ 
+static struct irq_chip virtirq_chip;
+
+/*
+ * evtchn_do_upcall
+ *
+ * This is the main entry point for processing IRQs and VIRQs for this domain.
+ *
+ * The function runs with IRQs OFF during the whole execution of the function. As such, this function is executed in top half processing of the IRQ.
+ * - All pending event channels are processed one after the other, according to their bit position within the bitmap.
+ * - Checking pending IRQs in AVZ (low-level IRQ) is performed at the end of the loop so that we can react immediately if some new IRQs have been generated
+ *   and are present in the GIC.
+ *
+ */
+void virq_handle(unsigned irq_nr)
+{
+	unsigned int evtchn;
+	int l1, virq;
+	int loopmax = 0;
+
+	BUG_ON(!irqs_disabled());
+
+retry:
+	l1 = xchg(&avz_shared->evtchn_upcall_pending, 0);
+
+	while (true) {
+		for (evtchn = 0; evtchn < NR_EVTCHN; evtchn++)
+			if ((avz_shared->evtchn_pending[evtchn]) && !evtchn_is_masked(evtchn))
+				break;
+
+		if (evtchn == NR_EVTCHN)
+			break;
+
+		BUG_ON(!per_cpu(evtchn_info, smp_processor_id()).valid[evtchn]);
+
+		loopmax++;
+
+		if (loopmax > 500)   /* Probably something wrong ;-) */
+			lprintk("%s: Warning trying to process evtchn: %d IRQ: %d for quite a long time on CPU %d / masked: %d...\n",
+				__func__, evtchn, per_cpu(evtchn_info, smp_processor_id()).evtchn_to_virq[evtchn], smp_processor_id(), evtchn_is_masked(evtchn));
+
+		virq = per_cpu(evtchn_info, smp_processor_id()).evtchn_to_virq[evtchn];
+
+		clear_evtchn(evtchn_from_virq(virq));
+
+		generic_handle_irq(VIRQ_BASE + virq);
+
+		BUG_ON(!irqs_disabled());
+	};
+
+	if (avz_shared->evtchn_upcall_pending)
+		goto retry;
+
+}
+
+static int find_unbound_virq(int cpu)
+{
+	int virq;
+
+	for (virq = 0; virq < NR_VIRQS; virq++)
+		if (per_cpu(evtchn_info, cpu).virq_bindcount[virq] == 0)
+			break;
+
+	if (virq == NR_VIRQS)
+		panic("No available IRQ to bind to: increase NR_IRQS!\n");
+
+	return virq;
+}
+
+static int bind_evtchn_to_virq(unsigned int evtchn)
+{
+	int virq;
+	int cpu = smp_processor_id();
+
+	spin_lock(&virq_mapping_update_lock);
+
+	if ((virq = per_cpu(evtchn_info, cpu).evtchn_to_virq[evtchn]) == -1) {
+		virq = find_unbound_virq(cpu);
+		per_cpu(evtchn_info, cpu).evtchn_to_virq[evtchn] = virq;
+		per_cpu(evtchn_info, cpu).virq_to_evtchn[virq] = evtchn;
+		per_cpu(evtchn_info, cpu).valid[evtchn] = true;
+
+	}
+
+	per_cpu(evtchn_info, cpu).virq_bindcount[virq]++;
+
+	spin_unlock(&virq_mapping_update_lock);
+
+	return virq;
+}
+
+void unbind_domain_evtchn(unsigned int domID, unsigned int evtchn)
+{
+	avz_hyp_t args;
+
+        args.cmd = AVZ_EVENT_CHANNEL_OP;
+        args.u.avz_evtchn.evtchn_op.cmd = EVTCHNOP_unbind_domain;
+
+        args.u.avz_evtchn.evtchn_op.u.bind_interdomain.remote_dom = domID;
+	args.u.avz_evtchn.evtchn_op.u.bind_interdomain.local_evtchn = evtchn;
+
+        avz_hypercall(&args);
+
+        evtchn_info.valid[evtchn] = false;
+}
+
+static int bind_interdomain_evtchn_to_virq(unsigned int remote_domain, unsigned int remote_evtchn)
+{
+        avz_hyp_t args;
+        int virq;
+
+ 	args.cmd = AVZ_EVENT_CHANNEL_OP;
+        args.u.avz_evtchn.evtchn_op.cmd = EVTCHNOP_bind_interdomain;
+	
+	args.u.avz_evtchn.evtchn_op.u.bind_interdomain.remote_dom = remote_domain;
+        args.u.avz_evtchn.evtchn_op.u.bind_interdomain.remote_evtchn = remote_evtchn;
+
+        avz_hypercall(&args);
+
+        virq = bind_evtchn_to_virq(args.u.avz_evtchn.evtchn_op.u.bind_interdomain.local_evtchn);
+
+	return virq;
+}
+
+int bind_existing_interdomain_evtchn(unsigned local_evtchn, unsigned int remote_domain, unsigned int remote_evtchn)
+{
+        avz_hyp_t args;
+        int virq;
+
+	args.cmd = AVZ_EVENT_CHANNEL_OP;
+        args.u.avz_evtchn.evtchn_op.cmd = EVTCHNOP_bind_existing_interdomain;
+
+	args.u.avz_evtchn.evtchn_op.u.bind_interdomain.local_evtchn = local_evtchn;
+	args.u.avz_evtchn.evtchn_op.u.bind_interdomain.remote_dom  = remote_domain;
+	args.u.avz_evtchn.evtchn_op.u.bind_interdomain.remote_evtchn = remote_evtchn;
+
+ 	avz_hypercall(&args);
+
+	virq = bind_evtchn_to_virq(args.u.avz_evtchn.evtchn_op.u.bind_interdomain.local_evtchn);
+
+	return virq;
+}
+
+static void unbind_from_virq(unsigned int virq)
+{
+        avz_hyp_t args;
+        int evtchn = evtchn_from_virq(virq);
+	int cpu = smp_processor_id();
+
+	spin_lock(&virq_mapping_update_lock);
+
+	if (--per_cpu(evtchn_info, cpu).virq_bindcount[virq] == 0) {
+
+		args.cmd = AVZ_EVENT_CHANNEL_OP;
+                args.u.avz_evtchn.evtchn_op.cmd = EVTCHNOP_close;
+
+                args.u.avz_evtchn.evtchn_op.u.close.evtchn = evtchn;
+
+		avz_hypercall(&args);
+
+		per_cpu(evtchn_info, cpu).evtchn_to_virq[evtchn] = -1;
+		per_cpu(evtchn_info, cpu).valid[evtchn] = false;
+	}
+
+	spin_unlock(&virq_mapping_update_lock);
+}
+
+int bind_evtchn_to_virq_handler(unsigned int evtchn, irq_handler_t handler, irq_handler_t thread_fn, unsigned long irqflags, const char *devname, void *dev_id)
+{
+	unsigned int virq;
+	int retval;
+
+	virq = bind_evtchn_to_virq(evtchn);
+ 
+	retval = request_threaded_irq(VIRQ_BASE + virq, handler, thread_fn, irqflags, devname, dev_id);
+	if (retval != 0) {
+		unbind_from_virq(virq);
+		return retval;
+	}
+
+	return virq;
+}
+
+int bind_interdomain_evtchn_to_virqhandler(unsigned int remote_domain, unsigned int remote_evtchn, irq_handler_t handler, irq_handler_t thread_fn, unsigned long irqflags, const char *devname, void *dev_id)
+{
+	int virq, retval;
+
+	DBG("%s: devname = %s / remote evtchn: %d remote domain: %d\n", __func__, devname, remote_evtchn, remote_domain);
+	virq = bind_interdomain_evtchn_to_virq(remote_domain, remote_evtchn);
+	BUG_ON(virq < 0);
+
+	if (handler != NULL) {
+		retval = request_threaded_irq(VIRQ_BASE + virq, handler, thread_fn, irqflags, devname, dev_id);
+		if (retval != 0)
+			BUG();
+	}
+
+	return virq;
+}
+
+void unbind_from_virqhandler(unsigned int virq, void *dev_id)
+{
+	struct irq_desc *desc = irq_to_desc(VIRQ_BASE + virq);
+
+	/* If we have a virq only to manage notification (without handler),
+	 * we should not free an "already-free" irq in Linux.
+	 */
+	if (desc->action)
+		free_irq(VIRQ_BASE + virq, dev_id);
+
+	unbind_from_virq(virq);
+
+}
+
+/*
+ * Interface to generic handling in irq.c
+ */
+static unsigned int startup_virtirq(struct irq_data *irq)
+{
+	int evtchn = evtchn_from_irq_data(irq);
+
+	unmask_evtchn(evtchn);
+	return 0;
+}
+
+static void shutdown_virtirq(struct irq_data *irq)
+{
+	int evtchn = evtchn_from_irq_data(irq);
+
+	mask_evtchn(evtchn);
+}
+
+static void enable_virtirq(struct irq_data *irq)
+{
+	int evtchn = evtchn_from_irq_data(irq);
+
+	unmask_evtchn(evtchn);
+}
+
+static void disable_virtirq(struct irq_data *irq)
+{
+	int evtchn = evtchn_from_irq_data(irq);
+
+	mask_evtchn(evtchn);
+}
+
+static struct irq_chip virtirq_chip = {
+		.name             = "avz_virt-irq",
+		.irq_startup      = startup_virtirq,
+		.irq_shutdown     = shutdown_virtirq,
+		.irq_enable       = enable_virtirq,
+		.irq_disable      = disable_virtirq,
+		.irq_unmask       = enable_virtirq,
+		.irq_mask         = disable_virtirq,
+};
+
+
+void notify_remote_via_virq(int virq)
+{
+	int evtchn = evtchn_from_virq(virq);
+
+	notify_remote_via_evtchn(evtchn);
+}
+
+void mask_evtchn(int evtchn)
+{
+	per_cpu(evtchn_info, smp_processor_id()).evtchn_mask[evtchn] = true;
+}
+
+void unmask_evtchn(int evtchn)
+{
+	per_cpu(evtchn_info, smp_processor_id()).evtchn_mask[evtchn] = false;
+}
+
+void virq_init(void)
+{
+	struct irq_desc *irqdesc; /* Linux IRQ descriptor */
+
+	int i, cpu;
+	cpumask_t cpumask = CPU_MASK_ALL;
+
+	/*
+	 * For each CPU, initialize event channels for all IRQs.
+	 * An IRQ will processed by only one CPU, but it may be rebound to another CPU as well.
+	 */
+
+	for_each_cpu(cpu, &cpumask) {
+ 
+		/* No event-channel -> IRQ mappings. */
+		for (i = 0; i < NR_EVTCHN; i++) {
+			per_cpu(evtchn_info, cpu).evtchn_to_virq[i] = -1;
+			per_cpu(evtchn_info, cpu).valid[i] = false;
+
+			per_cpu(evtchn_info, cpu).evtchn_mask[i] = true;
+		}
+
+#ifdef CONFIG_SPARSE_IRQ
+		irq_alloc_descs(VIRQ_BASE, VIRQ_BASE, NR_VIRQS, numa_node_id());
+#endif
+
+		/* Dynamic IRQ space is currently unbound. Zero the refcnts. */
+		for (i = 0; i < NR_VIRQS; i++)
+			per_cpu(evtchn_info, cpu).virq_bindcount[i] = 0;
+	}
+
+	/* Configure the irqdesc associated to VIRQs */
+	for (i = 0; i < NR_VIRQS; i++) {
+		irqdesc = irq_to_desc(VIRQ_BASE + i);
+
+		irqdesc->irq_data.common->state_use_accessors |= IRQD_IRQ_DISABLED;
+		irqdesc->status_use_accessors &= ~IRQ_NOREQUEST;
+
+		irqdesc->action = NULL;
+		irqdesc->depth = 1;
+		irqdesc->irq_data.chip = &virtirq_chip;
+
+		irq_set_handler(VIRQ_BASE + i, handle_level_irq);
+	}
+}
